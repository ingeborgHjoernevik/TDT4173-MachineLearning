{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kNN final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingeborgHjoernevik/TDT4173-MachineLearning/blob/main/kNN_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6k7-PRqQVro"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import time \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def load_data():\n",
        "  path1 = '/content/drive/My Drive/TDT4173 Maskinlæring /data_batch_1'\n",
        "  path2 = '/content/drive/My Drive/TDT4173 Maskinlæring /data_batch_2'\n",
        "  path3 = '/content/drive/My Drive/TDT4173 Maskinlæring /data_batch_3'\n",
        "  path4 = '/content/drive/My Drive/TDT4173 Maskinlæring /data_batch_4'  \n",
        "  path5 = '/content/drive/My Drive/TDT4173 Maskinlæring /data_batch_5'\n",
        "  path6 = '/content/drive/My Drive/TDT4173 Maskinlæring /test_batch'\n",
        "\n",
        "  listOfTestFiles = [path1, path2, path3, path4, path5, path6]\n",
        "\n",
        "  train = []\n",
        "  train_labels = []\n",
        "  test = []\n",
        "  test_labels = []\n",
        "\n",
        "  #For collecting Training data:\n",
        "  for file in listOfTestFiles[0:5]:\n",
        "      with open(file,'rb') as fo:\n",
        "          dict = pickle.load(fo,encoding='bytes')\n",
        "          train.append(dict[b'data'])\n",
        "          train_labels.append(dict[b'labels'])\n",
        "\n",
        "  #for collecting Testing data\n",
        "  with open(path6,'rb') as fo:\n",
        "          dict = pickle.load(fo,encoding='bytes')\n",
        "          test.append(dict[b'data'])\n",
        "          test_labels.append(dict[b'labels']) \n",
        "\n",
        "  dictData = {}\n",
        "  dictData['train_data'] = np.reshape(np.array(train),newshape=(np.array(train).shape[0]*np.array(train).shape[1],np.array(train).shape[2]))\n",
        "  dictData['train_labels'] = np.reshape(np.array(train_labels),newshape=(np.array(train_labels).shape[0]*np.array(train_labels).shape[1]))\n",
        "  dictData['test_data'] = np.reshape(np.array(test),newshape=(np.array(test).shape[0]*np.array(test).shape[1],np.array(test).shape[2]))\n",
        "  dictData['test_labels'] = np.reshape(np.array(test_labels),newshape=(np.array(test_labels).shape[0]*np.array(test_labels).shape[1]))\n",
        "  return dictData\n",
        "\n",
        "  #Run pca to find number of components d that have >p variance\n",
        "def find_components(p, train_x):\n",
        "  starttime = time.time()\n",
        "  pca = PCA()\n",
        "  pca.fit(train_x)\n",
        "  cum_sum = np.cumsum(pca.explained_variance_ratio_, axis = 0)\n",
        "  out = np.cumsum(pca.explained_variance_ratio_)\n",
        "  top_percentile = np.where(cum_sum>p)[0][0]\n",
        "  print(\"Top \",p*100, \"% is gained by choosing: \", top_percentile, \"components. Time used: \", time.time()-starttime)\n",
        "  return top_percentile\n",
        "\n",
        "  #View image\n",
        "def view(image):\n",
        "  temp = image\n",
        "  if (image.shape[0]==3072): #For RGB images\n",
        "    R = temp[0:1024].reshape(32,32)\n",
        "    G = np.reshape(temp[1024:2048],newshape=(32,32))\n",
        "    B = np.reshape(temp[2048:],newshape=(32,32))\n",
        "    temp = np.dstack((R,G,B))   #for stacking all these 32,32 matrices.\n",
        "    plt.imshow(temp)\n",
        "  else: #For grayscale images\n",
        "    temp = temp.reshape(32,32)\n",
        "    plt.imshow(temp, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "#Transform RGB to gray values. Code from https://github.com/ppplinday/Picture-Classification-PCA-KNN/blob/master/KNN-PCA.py\n",
        "def RGB_to_gray(data):\n",
        "  data = data.reshape(data.shape[0],3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
        "  newdata = np.zeros([data.shape[0], data.shape[1], data.shape[2]], float)\n",
        "  num    = data.shape[0]\n",
        "  height = data.shape[1]\n",
        "  width  = data.shape[2]\n",
        "  for n in range(num):\n",
        "      for row in range(height):\n",
        "          for col in range(width):\n",
        "              newdata[n, row, col] = 0.299 * data[n, row, col, 0] + 0.587 * data[n, row, col, 1] + 0.114 * data[n, row, col, 2]\n",
        "  newdata = newdata.reshape(data.shape[0], 32*32)\n",
        "  return newdata\n",
        "\n",
        "#Transform into d components\n",
        "def transform_to(d, X):\n",
        "  starttime = time.time()\n",
        "  p_pca = PCA(d)\n",
        "  out = p_pca.fit_transform(X)\n",
        "  print(\"Time used on making pca: \", time.time()-starttime)\n",
        "  return out\n",
        "\n",
        "#Making the knn with k neighbours\n",
        "def make_knn(k, X, Y):\n",
        "  starttime = time.time()\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn = knn.fit(X, Y)\n",
        "  print(\"Time used on making knn: \", time.time()-starttime)\n",
        "  return knn\n",
        "\n",
        "#Calculating the accuracy of the knn model on a training set X with labels Y\n",
        "def calc_accuracy(knn, X, Y):\n",
        "  starttime = time.time()\n",
        "  accuracy = knn.score(X,Y)\n",
        "  #print(\"Accuracy on validation set with k = \", knn.n_neighbors , \": \", accuracy)\n",
        "  print(\"Time used on calculating accuracy: \", time.time()-starttime)\n",
        "  return accuracy\n",
        "\n",
        "#Makes a new sheet in the \"accuracy\"-workbook, with a name based on number of features and the length of the validation set.\n",
        "def newSheet(d, val_x):\n",
        "  wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1iqm61ROBjpCUysro7AoT0yrUd6z57VmVcPQ2FlyiHXA/edit#gid=0')\n",
        "  title = \"N:\"+str(d)+\".L:\"+str(val_x.shape[0])\n",
        "  rows = 100\n",
        "  col = 2\n",
        "  makeNew = True\n",
        "  for ws in wb.worksheets():\n",
        "    if (str(ws).split()[1]==str(\"'\"+title+\"'\")):\n",
        "      sheet = ws\n",
        "      data = ws.get_all_values()\n",
        "      makeNew = False\n",
        "  if makeNew:\n",
        "    sheet = wb.add_worksheet(title, rows, col)\n",
        "    data = {'K':[0]*rows, 'Accuracy (%)':[0]*rows}\n",
        " \n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = df.iloc[0]\n",
        "  df = df.iloc[1:]\n",
        "  set_with_dataframe(sheet, df)\n",
        "  return sheet, df\n",
        "\n",
        "\n",
        "#Calculating which k that gives the best accuracy on a data set val_x, when it is trained on train_x. These values are saved in a spreadsheet called \"Accuracy\".\n",
        "def calculate_best_k(d, train_x, train_y, val_x, val_y):\n",
        "  best_k = 1\n",
        "  best_acc = 0.1\n",
        "  \n",
        "  sheet, df = newSheet(d, val_x)\n",
        "\n",
        "  for k in range(1, 60):\n",
        "    knn = make_knn(k, train_x, train_y)\n",
        "    acc = calc_accuracy(knn, val_x, val_y)\n",
        "\n",
        "    df.iloc[k-1][0]=k\n",
        "    df.iloc[k-1][1]=acc*100\n",
        "    print(\"k = \", k, \" gir acc = \", acc,\"\\n\")\n",
        "    set_with_dataframe(sheet, df)\n",
        "\n",
        "    if (acc>best_acc):\n",
        "      best_k = k\n",
        "      best_acc = acc\n",
        "  return best_k, best_acc\n",
        "\n",
        "#Predicts the classes of a set X\n",
        "def pred(knn, X):\n",
        "  starttime=time.time()\n",
        "  out = knn.predict(X)\n",
        "  print(\"Time used on predicting dataset: \", time.time()-starttime)\n",
        "  return out\n",
        "\n",
        "\n",
        "#The main method for finding the best k and its accuracy for the test set. This method is for using both the colour-to-gray-conversion and the use of PCA.\n",
        "def main():\n",
        "  #Load dataset\n",
        "  dataset = load_data()\n",
        "  print(\"Dataset loaded...\")\n",
        "\n",
        "  #Split dataset to training, validating and testing data.\n",
        "  x_train, y_train, x_test, y_test = dataset['train_data'],dataset['train_labels'],dataset['test_data'],dataset['test_labels']\n",
        "  train_x, train_y = x_train[0:45000],y_train[0:45000]\n",
        "  val_x, val_y = x_train[45000:],y_train[45000:]\n",
        "  print(\"Dataset splitted...\")\n",
        "\n",
        "  #Convert color images to gray images\n",
        "  gray_train_x = RGB_to_gray(train_x)\n",
        "  gray_val_x = RGB_to_gray(val_x) \n",
        "  print(\"Images converted...\")\n",
        "\n",
        "  #Find right number of components, according to a variance p\n",
        "  p = 0.95\n",
        "  d = find_components(p, gray_train_x)\n",
        "\n",
        "  #Make PCA\n",
        "  pca_train_x = transform_to(d, gray_train_x)\n",
        "  pca_val_x = transform_to(d, gray_val_x) \n",
        "\n",
        "  #Calculate best k by making kNNs an checking accuracy\n",
        "  best_k, best_acc = calculate_best_k(d, gray_train_x, train_y, gray_val_x, val_y)\n",
        "  \n",
        "  print(\"Best k is \", best_k, \" with an accuracy of \", best_acc)\n",
        "  \n",
        "\n",
        "main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}